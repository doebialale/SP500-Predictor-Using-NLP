{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cbcea67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import yfinance as yf\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29832fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def collect_headlines(date):\n",
    "    headlines = []\n",
    "    formatted_date = datetime.strptime(date, '%Y-%m-%d').strftime('%Y-%m-%d')\n",
    "\n",
    "    # Example URL structure; you need to find the correct URL structure for each source\n",
    "\n",
    "    # Scraping CNBC headlines\n",
    "    cnbc_url = f\"https://www.cnbc.com/us-economy?date={formatted_date}\"\n",
    "    response = requests.get(cnbc_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    cnbc_headlines = soup.find_all('a', class_='Card-title')\n",
    "    for headline in cnbc_headlines:\n",
    "      headlines.append({'source': 'CNBC', 'text': headline.get_text(strip=True)})\n",
    "\n",
    "    # Scraping Guardian headlines\n",
    "    guardian_url = f\"https://www.theguardian.com/money?date={formatted_date}\"\n",
    "    response = requests.get(guardian_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    guardian_headlines = soup.find_all('a', class_='dcr-lv2v9o')\n",
    "    for headline in guardian_headlines:\n",
    "        headlines.append({'source': 'Guardian', 'text': headline.get_text(strip=True)})\n",
    "\n",
    "    # Scraping Reuters headlines\n",
    "    #reuters_url = f\"https://www.reuters.com/markets/us/?date={formatted_date}\"\n",
    "    #response = requests.get(reuters_url)\n",
    "    #soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    #reuters_headlines = soup.find_all('a', {'Title': 'headline'})\n",
    "    #for headline in reuters_headlines:\n",
    "      #headlines.append({'source': 'Reuters', 'text': headline.get_text(strip=True)})\n",
    "   \n",
    "    return headlines\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9290f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_spy_data(date):\n",
    "    # Ensure date is a datetime object\n",
    "    if isinstance(date, str):\n",
    "        date = datetime.strptime(date, '%Y-%m-%d')\n",
    "    spy = yf.Ticker(\"SPY\")\n",
    "\n",
    "    # Define the start and end dates\n",
    "    start_date = date.strftime('%Y-%m-%d')\n",
    "    end_date = (date + timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "\n",
    "    data = spy.history(start=start_date, end=end_date)  # Get data for the specific date\n",
    "    if not data.empty:\n",
    "        # Calculate the difference as a percentage of the 'Open' price and add as a new column\n",
    "        data['Difference (%)'] = ((data['Close'] - data['Open']) / data['Open']) * 100\n",
    "        \n",
    "        # Convert the relevant data to a dictionary and return\n",
    "        return data[['Open', 'Close', 'Difference (%)']].reset_index().to_dict(orient='records')\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "280dfbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect_data_with_spy(date):\n",
    "    try:\n",
    "        # Collect headlines for the given date\n",
    "        headlines = pd.read_csv(\"../data/combined_headlines_new.csv\")  \n",
    "\n",
    "        # Convert date string to datetime object\n",
    "        headline_date = datetime.strptime(date, '%Y-%m-%d')\n",
    "\n",
    "        # Fetch SPY data for the given date\n",
    "        spy_data = fetch_spy_data(headline_date)\n",
    "\n",
    "        # Return collected data as a dictionary\n",
    "        return {\n",
    "            'headlines': headlines,\n",
    "            'spy_data': spy_data\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return {\n",
    "            'headlines': pd.DataFrame(),\n",
    "            'spy_data': []\n",
    "        }\n",
    "    \n",
    "    print(headlines)\n",
    "    print(spy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce9fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare SPY data\n",
    "spy_data = []\n",
    "for date in pd.date_range(start='2018-03-20', end='2020-07-17'):\n",
    "    result = collect_data_with_spy(date.strftime('%Y-%m-%d'))\n",
    "    spy_data.extend(result['spy_data'])\n",
    "\n",
    "spy_df = pd.DataFrame(spy_data)\n",
    "spy_df['date'] = pd.to_datetime(spy_df['Date']).dt.tz_localize(None)  # Remove timezone info\n",
    "spy_df.rename(columns={'Date': 'date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24f09187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SPY DataFrame Columns:\n",
      "Index(['date', 'Open', 'Close', 'Difference (%)'], dtype='object')\n",
      "SPY DataFrame:\n",
      "        date        Open       Close  Difference (%)\n",
      "0 2018-03-20  244.465486  244.474518        0.003694\n",
      "1 2018-03-21  244.429416  244.005341       -0.173496\n",
      "2 2018-03-22  241.731570  237.905884       -1.582617\n",
      "3 2018-03-23  238.357023  232.835007       -2.316700\n",
      "4 2018-03-26  236.516313  239.205109        1.136833\n"
     ]
    }
   ],
   "source": [
    "# Remove any duplicate columns if they exist\n",
    "spy_df = spy_df.loc[:, ~spy_df.columns.duplicated()]\n",
    "spy_df['date'] = pd.to_datetime(spy_df['date']).dt.tz_localize(None)\n",
    "\n",
    "# Print DataFrames for debugging\n",
    "print(\"\\nSPY DataFrame Columns:\")\n",
    "print(spy_df.columns)\n",
    "print(\"SPY DataFrame:\")\n",
    "print(spy_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16e28409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare headline data\n",
    "headline_data = []\n",
    "for date in pd.date_range(start='2018-03-20', end='2020-07-17'):\n",
    "    result = collect_data_with_spy(date.strftime('%Y-%m-%d'))\n",
    "    for headline in result['headlines'].to_dict('records'):\n",
    "        headline_data.append({\n",
    "            'date': date,          \n",
    "            'headline': headline.get('headlines', '')  # Use .get() to handle missing keys\n",
    "        })\n",
    "headline_df = pd.DataFrame(headline_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee2a9e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               date                                           headline\n",
      "0        2018-03-20  Jim Cramer : A better way to invest in the Cov...\n",
      "1        2018-03-20    Cramer's lightning round : I would own Teradyne\n",
      "2        2018-03-20  Cramer's week ahead : Big week for earnings , ...\n",
      "3        2018-03-20  IQ Capital CEO Keith Bliss says tech and healt...\n",
      "4        2018-03-20  Wall Street delivered the 'kind of pullback I'...\n",
      "...             ...                                                ...\n",
      "45383825 2020-07-17  Malaysia says never hired British data firm at...\n",
      "45383826 2020-07-17  Prosecutors search Volkswagen headquarters in ...\n",
      "45383827 2020-07-17   McDonald's sets greenhouse gas reduction targets\n",
      "45383828 2020-07-17  Pratt & Whitney to deliver spare A320neo engin...\n",
      "45383829 2020-07-17  UK will always consider ways to improve data l...\n",
      "\n",
      "[45383830 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Remove any duplicate columns if they exist\n",
    "headline_df = headline_df.loc[:, ~headline_df.columns.duplicated()]\n",
    "headline_df['date'] = pd.to_datetime(headline_df['date']).dt.tz_localize(None)\n",
    "print(headline_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d049938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge headline data with SPY data\n",
    "merged_df = pd.merge(headline_df, spy_df, on='date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0df5f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final dataset\n",
    "merged_df.to_csv('final_dataset_with_stock_price.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "440f1647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                date                                           headline  \\\n",
      "0         2018-03-20  Jim Cramer : A better way to invest in the Cov...   \n",
      "1         2018-03-20    Cramer's lightning round : I would own Teradyne   \n",
      "2         2018-03-20  Cramer's week ahead : Big week for earnings , ...   \n",
      "3         2018-03-20  IQ Capital CEO Keith Bliss says tech and healt...   \n",
      "4         2018-03-20  Wall Street delivered the 'kind of pullback I'...   \n",
      "...              ...                                                ...   \n",
      "232252145 2020-07-17  UK will always consider ways to improve data l...   \n",
      "232252146 2020-07-17  UK will always consider ways to improve data l...   \n",
      "232252147 2020-07-17  UK will always consider ways to improve data l...   \n",
      "232252148 2020-07-17  UK will always consider ways to improve data l...   \n",
      "232252149 2020-07-17  UK will always consider ways to improve data l...   \n",
      "\n",
      "                Close  \n",
      "0          244.474564  \n",
      "1          244.474564  \n",
      "2          244.474564  \n",
      "3          244.474564  \n",
      "4          244.474564  \n",
      "...               ...  \n",
      "232252145  303.290283  \n",
      "232252146  303.290344  \n",
      "232252147  303.290344  \n",
      "232252148  303.290405  \n",
      "232252149  303.290344  \n",
      "\n",
      "[232252150 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print the final DataFrame for verification\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb2d59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
